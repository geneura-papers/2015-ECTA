\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may
                            % need BEFORE the SCITEPRESS.sty package.
\usepackage{hyperref}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}
<<setup, cache=FALSE,echo=FALSE>>=
library("ggplot2")
library("RCurl") #To download stuff directly from the GitHub repo
library(e1071) # for skewness and kurtosis
# Now download stuff from noisy-ga repo
made.data <- read.csv(text = getURL("https://raw.githubusercontent.com/JJ/noisy-ga/master/data/MADE/made-data.csv"))
pacman.data <-  read.csv(text = getURL("https://raw.githubusercontent.com/JJ/noisy-ga/master/data/ms-pacman/pacman-fitness.csv"))
planetwars.data <- read.csv(text = getURL("https://raw.githubusercontent.com/JJ/noisy-ga/master/data/planet-wars/planetwars-fitness.csv"))
@ 

\title{There is Noisy Lunch: a study of noise in evolutionary optimization problems.}
% Antonio - ¿se pone algo de juegos en el título?
% MADE no es un juego y vamos a intentar hacerlo de la forma más general.

\author{\authorname{Juan J. Merelo\sup{1}, 
    Federico Liberatore,
    Antonio Fernández Ares, 
    Rubén García\sup{2}, 
    Zeineb Chelly, 
    Carlos Cotta, 
    Nuria Rico, 
    Antonio M. Mora\sup{1}
    and Pablo Garc{\'i}a-S{\'a}nchez\sup{1}} %FERGU: me pongo el último, que he sido el que menos ha hecho :P
% Antonio - Faltan las afiliaciones de mucha gente
  % Añadidlas, por favor. - JJ
\affiliation{\sup{1}CITIC and University of Granada}
\affiliation{\sup{2}Escuela de Doctorado, UGR}
\email{{\tt jmerelo@geneura.ugr.es}}
}

\keywords{games, evolutionary optimization, noise, uncertainty, noisy fitness}
% Antonio - yo pondría 'noisy fitness'
% Haberla puesto! Ya la he añadido.

\abstract{Noise or uncertainty appear in many optimization processes
  when there is not a single measure of optimality or fitness but a
  random variable representing it. These kind of problems
  have been known for a long time, but there has been no investigation
  of the statistical distribution those random variables follow,
  assuming in most cases that it is distributed normally and, thus,
  it can be modelled via an additive or multiplicative noise on top of
  a {\em non-noisy} fitness.
% Antonio - se modela con ruido o con una función?
  % Con ruido - JJ
In this paper we will look at several uncertain optimization problems
that have been addressed by means of Evolutionary Algorithms
% Antonio - añado una cosilla para aclarar que son evolutivos
% no hacía falta y creo que las conclusiones serían más o menos
% generales - JJ
 and prove that there is no single statistical model the evaluations
 of the fitness functions
% Antonio - yo pondría 'these variables' -> 'the fitness functions'
 % ni pat i ni pa mi - JJ
  follow, being
% Antonio - with -> being?
  % correcto - JJ
different not only from one problem to the next, but in
different phases of the optimization in a single problem.}

\onecolumn \maketitle \normalsize \vfill

% ******************************************************************************

\section{\uppercase{Introduction}}
\label{sec:introduction}

Optimization methods usually need a crisp and fixed value to work
correctly. This value, usually called {\em cost} or {\em fitness},
informs the procedure on how good is the solution and is used to
select particular solution over others. This does not imply that these
methods need a single floating point number; since they are based on
comparisons, it is usually enough that the values can be partially
ordered. Multiobjective optimization, for instance, just need to know
when 
comparing two solutions whether one or the other is the best or there
can be no comparison between them. In either case, the answer to the
question ``Is this solution better than the other?'' needs to be either a crisp
`Yes' or `No', or simply ``Impossible to know''.

In many cases, however, the fitness or cost of a solution cannot be
described by a crisp value. In those cases where there is \textit{uncertainty} in the measure, that is, in most real world, physical cases, or in the procedure 
% Antonio - 'physical' en qué sentido?
used to evaluate the solution, for instance, when using a stochastic procedure
to make that measure, the best way to describe a solution will be a random
variable, not a single, or even a vector, value. 
In our research we have found this happens in many different
optimization problems:
\begin{itemize}
  \item When optimizing the layout of a webpage using Simulated
  Annealing (SA) \cite{jj-ppsn98}. Since SA is a stochastic procedure, the fitness obtained by a solution will be a random variable.
\item When training any kind of neural network, such as those in
  \cite{esann94,merelo:ESNN}; in the second case we dealt with a
  physical installation, introducing another kind of randomness. Since training a neural network is a   stochastic procedure, the error rate obtained after every training run will also follow  a statistical distribution.
% Antonio - seguir una distribución estadística es lo mismo que ser una variable aleatoria?
\item When evolving game bots (autonomous agents) \cite{bots:evostar}. In this case, the
  uncertainty arises from the problem itself; in games, several factors such as the initial positions of the players or the opponent's behavior add certain stochastic component so that final score will also be {\em uncertain} or {\em noisy}; in some cases, too, the bot itself will rely on probabilities to
  generate its behavior \cite{EvoStar2014:CoEvolutionary}, in which
  case two different runs with exactly the same initial conditions and
  opponent will also yield different scores.
\end{itemize}

In all these cases it cannot be said that there is {\em noise} added to
a {\em crisp} fitness. The fitness itself is a statistical variable
whose value arises from a stochastic process, evaluation or
training, however, we have not seen an exhaustive research of the
behavior of fitness as a random variable.

That is why, after some initial study of noise in
\cite{merelo14:noisy}, 
% Antonio - indicar en qué tipo de problema se estudió. Crees que aquí casaría el estudio de ruido que hicimos en Planet Wars? La referencia que he incluído en el .bib (NoisyFitness_JCST)
where our findings indicated that, in some cases, noise followed a Gamma, that
is a skewed normal distribution and proposing a solution to this using
Wilcoxon comparison as a selection operator, 
% Antonio - referencia a esto
we dug into data discovering that, even if the distribution in that particular case was always a gamma, the parameters of the distribution were different,
which meant that the random variable behaved in different ways
depending on the particular individual, the state of evolution and, of
course, the particular problem.

This initial conclusion disagrees with the usual assumptions in
optimization in uncertain environments, where it is frequent to assume
that the noise is normally distributed and with a fixed sigma
\cite{arnold2001evolution}. For instance, in the Black Box
Optimization Benchmarks \cite{hansen2009real} the uncertainty was
simulated by adding noise centered in 0 and with a Cauchy distribution
with different widths. 
% Antonio - Referencia?
Either multiplicative or additive noise has
been used in different occasions. 

That is why in this paper we have collected data from three different
problems 
% Antonio - explicar un poco los problemas considerados (cogerlo de los artículos pertinentes). ;D
and tried to find a model for the fitness using statistical
tools. Our aim is to eventually find a model that is as general
as possible and that is able to account for most sources of
uncertainty; failing that, to try and find selection operators that
are able to work with random variables in a natural way. However, this
is not the focus of this paper and, if it is eventually needed, is
left as future work. 

The rest of the paper is organized as follows. Next we present the
state of the art in evolutionary algorithms in uncertain environments,
to be followed by a short presentation of the three problems with
uncertainty whose measures will be used in this paper in Section
\ref{sec:problems}. Results will be presented in Section \ref{sec:res}, followed by our conclusions.

% ******************************************************************************

\section{\uppercase{State of the Art}}
\label{sec:soa}

\sloppypar The most recent and comprehensive review of the state of the art in 
% Antonio - el más reciente es de 2005?
% Pues sí. No he encontrado más. - JJ
evolutionary algorithms in {\em uncertain} environments 
was done by \cite{Jin2005303}, although recent
papers such as \cite{DBLP:journals/corr/QianYZ13,6931307} and
\cite{Qian:sampling} include brief updates. In that first survey  
the authors state that uncertainty is categorized into noise,
robustness issues, fitness approximation, and time-varying fitness functions, and then, different options for dealing with it are proposed. In principle, the
approach presented in this paper was designed to deal with the first kind of
uncertainty, noise or uncertainty in fitness evaluation, although it could be argued that there is uncertainty in the true fitness as stated in the third
category; however, we do not think that is the case and, in general,
that third issue refers to the case in which expensive fitness
functions are substituted by surrogate functions which carry a certain
amount of error. They suggest several methods, based either on using
averaging or using a selection threshold over which one or other
individual is selected. But since then, several other solutions have
been proposed.

\sloppypar For scientists not concerned on solving the problem of noise, but on
a straightforward solution of the optimization problem without
modification of existing tools and methodologies, a usual approach is
just to disregard the fact that the fitness is noisy and use whatever
value is returned by a single evaluation or after re-evaluation each
generation. This was the option in our previous research in
games although one evaluation in some of those works consists, in
  fact, in an average of several evaluations, in different maps or
  considering different opponents, for instance. 
\cite{bots:evostar,DBLP:journals/jcst/MoraFGGF12,liberatore:pacman}  
% Antonio - esto habría que comentarlo, ya que, normalmente, no nos
% limitamos a evaluar una vez y ya, sino a hacer varias evaluaciones
% en distintos mapas o contra distintos rivales. Lo he puesto como
% nota al pie, pero quizá habría que meterlo en el texto y explicarlo
% mejor, aunque sea para decir que no es válido. ;)
% Añadido. NO se usan ni notas al pie ni paréntesis si se puede evitar
% - JJ
and evolution of neural networks \cite{castilloGECCO99,merelo:ESNN} and leads, if the population is large enough, to an {\em implicit averaging} as
mentioned in \cite{Jin2005303}.

In fact, selection used in evolutionary algorithms
is also stochastic, so noise in fitness evaluation
will have the same effect as randomness in selection or a higher mutation
rate, which might make the evolution process easier and not harder
in some particular cases
\cite{DBLP:journals/corr/QianYZ13}. 
% Antonio - yo estoy lo pongo en duda, por lo menos en el caso de los
% juegos, aunque existe un factor de ruido, no se corresponde con algo
% aleatorio (o casi), a menos que los rivales estén muy igualados o
% que se produzcan situaciones MUY extrañas en el mapa. Normalmente
% los buenos ganarán a los malos y viceversa. 
% el ruido por definición es aleatorio y de eso no es de lo que estoy
% hablando en ese párrafo, sino del trabajo de esas personas - JJ 
In fact, Miller and Goldberg proved that an infinite population would not
be affected by noise \cite{miller1996genetic} and Jun-Hua and Ming studied the
effect of noise in convergence rates \cite{Junhua20136780}, proving
that an elitist genetic algorithm finds at least one solution, although with a lowered convergence rate. 

But real populations are finite, so the usual approach to dealing with
fitness with a degree of randomness is to increase the population size
to a value bigger than would be needed in a non-noisy environment.  
% Antonio - en juegos el tratamiento es la repetición de evaluaciones
% (creo recordar que hay algunos trabajos que lo indican así, aparte
% del nuestro de la revista JCST). :D    Vale, esto lo pone debajo...  
% resuelto entonces - JJ
In fact, it has been recently proved that using {\em sex}, that is, crossover, 
% Antonio - esto qué es? cómo los carteles esos que ponen en grande SEXO para llamar la atención? :D
% también ejerce ese papel en la naturaleza: reducir ruido - JJ
is able to deal successfully with noise \cite{2015arXiv150202793F}, while an
evolutionary algorithm based mainly on mutation,
%CARLOS: un algorithm mu+1 puede usar cruce - elimino esto  
such as the $\mu$+1 EA, or evolutionary programming, 
% but that's the one mentioned in the paper, so I put it back - JJ
would suffer a considerable degradation of performance. 
However, crossover is part of the standard kit of evolutionary
algorithms, so using it and increasing the population size has the
advantage that no special provision or change in the implementation
has to be made, just different values of the standard parameters.

Another more theoretically sound way is using a statistical central tendency
indicator, which is usually the {\em average}; which happens to be
equal to the median in the case of the random variable following the
normal distribution. This strategy is called
{\em explicit averaging} by Jin and Branke and is used, for instance,
in \cite{Junhua20136780}. Averaging decreases the variance of fitness but
the problem is that it is not clear in advance what would be the
sample size used for averaging \cite{aizawa1994scheduling}. We have
used it in some cases but in a different way: not re-evaluating
individuals every additional generation and computing the average but
computing the fitness using the average of several evaluations,
usually five or more \cite{DBLP:journals/jcst/MoraFGGF12}. 
Most authors use several measures of fitness for each new individual
\cite{costa2013using}, although other averaging strategies have also
been proposed, like averaging over the neighbourhood of the
individual or using {\em resampling}, that is, more measures of fitness in a
number which is decided heuristically
\cite{liu2014mathematically}. This assumes that there is, effectively,
an average of the 
fitness values which is true for Gaussian random noise and other
distributions such as Gamma or Cauchy, but not
necessarily for all distributions. 

To the best of our knowledge, 
other measures like the median which might be more adequate for
certain noise models, but which is the same for the normal
distribution usually attributed to noise, have not been tested; the median always exists,
while the average might not exist for non-centrally distributed
variables. Besides, most models keep the number of evaluations fixed 
and independent of its value, 
which might result in bad individuals
being evaluated many times before being discarded; some authors have
proposed {\em resampling}, that is, re-evaluate the individuals a number of times to increase the precision in fitness
\cite{RadaVilela2014,6900521},
% Antonio - el resampling se ha comentado también unas líneas antes 
% y? Qué sugieres? - JJ
which will effectively increase the number of
evaluations and thus slow down the search. In any case, using average is
also a small change to the algorithm framework, requiring only
using as new fitness function the average of several evaluations.

These two approaches that are focused on the evaluation process might
be complemented with changes to the selection process. For instance,
using a threshold \cite{Rudolph2001318,6900521} that is related to the noise characteristics to
avoid making comparisons of individuals that might, in fact, be very
similar or statistically the same; this is usually called {\em
  threshold selection} and can be applied either to explicit or
implicit averaging fitness functions. The algorithms used for
solution, themselves, can be also tested, with some authors proposing, instead of taking more measures, 
testing different solvers \cite{cauwet2014algorithm}, some of which
might be more affected by noise than others. However, recent papers
have proved that sampling might be ineffective \cite{Qian:sampling} in
some types of evolutionary algorithms, adding running time without an
additional benefit in terms of performance. This is one lead we will
use in the current paper. 

Any of these approaches do have the problem of statistical
representation of the {\em true} fitness, even more so if there is not
such a thing, but several measures that represent, {\em as a set}
the fitness of an individual. This is what we are going to use in this
paper, where we present a method that uses resampling via an
individual memory and use either explicit averaging or statistical
tests like the non-parametric Wilcoxon test. 
First we will examine and try to find the shape of the noise that
actually appears in games and other problems; 
% Antonio - aclarar lo de 'games' => "in optimization of behavioural models of autonomous agents for playing games"
% añadido "and other problems" - JJ
then we will check in this paper what is the influence on the quality of
results of these two strategies and which one, if any, is the best
when working in noisy environments.  

% ******************************************************************************

\section{Problems used in this paper}
\label{sec:problems}


The fitness of three different problems, all of them related to
computational intelligence in games has been used in this paper:
optimization of the ghost team in Ms. Pac-Man, which will be described
in subsection \ref{ss:pacman}, generation of character backstories in
artificial worlds \ref{ss:made} and optimization of bots for playing
the real time strategy game Planet Wars \ref{ss:pw}. These three
problems have been chosen for several reasons, the most important of
which is that we have been working on them and thus have data
available; another reason is that the origin of the uncertainty is
different in the three cases. In the case of Ms. Pac-Man, it is due to
the nature of the game; in the case of MADE, fitness is computed
through a simulation; finally, in the case of Planet Wars, the bot
themselves have a random component, with its representation including
probabilities of different courses of action. It is not a complete
representation of all causes of uncertainty, but the sample is big
enough so that we can generalize the results obtained, which will be
presented in the next section. 


\subsection{Ghost team optimization}
\label{ss:pacman}


Pac-Man game, released in 1981, has resulted in several variants. One of the most famous Ms. Pac-Man, that extends the mechanics of the original game with several features. 
The most interesting is the possibility of applying different strategies to the ghosts’ behavior. Another one is the inclusion of a random movement to prevent the use of patterns to clear each round.

Recently, researchers are paying more attention to this game due to the Ms. Pac-Man vs Ghosts competition, where participants can submit controllers for both Ms. Pac-Man and the Ghost Team. 
In this competition, the objective of Ms. Pac-Man is to maximize the final score, while the objective of the Ghost Team is to minimize it. 

Proposed implementation follows the following restrictions for the Ghost Team:
\begin{itemize}
 \item A ghost can never stop and if it is in a corridor it must move forward.
 \item A ghost can choose its direction only at a junction.
 \item Every time a ghost is at a junction the controller has to provide a direction from the set of feasible directions.
 \item After 4000 game ticks, a level is considered completed and the game moves on to the next level.
\end{itemize}


\subsection{Creation of character backstories}
\label{ss:made}

MADE (Massive Drama Engine for non-player characters)
\cite{garcia14my} is a framework for the automatic generation of
virtual worlds that allow the emergence of backstories for secondary
characters that can later on be included in videogames. In this context, an archetype
is a well-known behaviour present in the imaginary collective (for
example, a ``hero'' or a ``villain''). Given a fitness to model the
existence of different $N_a$ archetypes for a virtual world, MADE uses
a genetic algorithm to optimize the parameter values of a Finite State
Machine (FSM) that model the agents of that world. For the evaluation,
a world is simulated using this parameter set, and the log is analysed
to detect behaviours of the world agents to match with the desired
archetypes. 

As the evolved parameters are the probabilities to jump from one state
to another in the FSM, each fitness evaluation is performed executing
the virtual world five times per individual, obtaining the average
fitness. Selection is, therefore, performed comparing this average
fitness, using a binary tournament in this case. Fitness values range
from 0 and $N_a$ and are calculated taking into account the rate of
the archetypes in the execution log. 


\subsection{Real time strategy: Planet Wars}
\label{ss:pw}

Planet Wars \cite{DBLP:conf/cec/Fernandez-AresMGGF11} is a
simple Real-Time strategy (RTS) game. RTS games are not turn-based and
their objective is to defeat the enemy using resources available in
the map to build units and structures. 

Computational intelligence
methods have been applied to Planet Wars since it provides a
simplification of the elements of the RTS: one kind of units
(spaceships) and one kind of resources and structures
(planets). Spaceships are automatically generated in the planets owned by the player
and they are used to conquer the enemy planets, as this is the objective of the game. 

In this paper we are
using the results obtained from the Genebot algorithm \cite{EvoStar2014:GPBot}. This
algorithm optimizes the parameters of a hand-coded FSM that indicates how many ships send from each
planet to attack or reinforce another planet depending of some other values (such as the distance between planets). The generated bot is not
deterministic, as some of the jumps of the states are based in
probabilities. 
Fitness is calculated confronting five times the bot obtained from the parameter set of the FSM against
a competitive hand-coded bot. The result of each match takes into account the `slope' of the number of player spaceships
during the time of the match. Positive results mean that the bot won, as the slope will be positive, and vice versa. Theoretical values are in the range $[-1,1]$, although these values are impossible to attain in the game. A value of -1 would indicate that the player lost all their ships in the initial time, while $1$ would mean the contrary: it generated all the spaceships and won in the initial time. The fitness of an individual is the sum of all five results, and therefore being in the range $[-5,5]$. This fitness has been explained in more detail in \cite{Fernandez-Ares_COSECIVI14}.


% ******************************************************************************

\section{Experiments and Results}
\label{sec:res}

With the problems presented above, data on fitness was collected by
selecting a few random individuals in every generation and measuring
its fitness 100 times. Thus, every individual is represented by a
random variable with the 100 measures taken with its
fitness. According to the usual assumptions, this random variable
should follow a normal distribution, with probably different $\sigma$
and centered on the {\em true} fitness value. In order to check that
hypothesis, we plotted the {\em skewness}, that is, assymmetry of the
distribution, and kurtosis, which is a parameter related to the shape
of the distribution. A symmetrical distribution like the normal
distribution has a skewness and kurtosis equal to 0; assymetric
distributions, such as the Gamma that we had found in previous papers
\cite{merelo14:noisy}, has non-zero skewness and kurtosis which are
related to their $\alpha$ and $\kappa$ parameters, for instance. Any
random variable has skewness and kurtosis at any rate, and we have
computed and plotted them in the next figures, where skewness is
plotted as $x$ axis against kurtosis in the $y$ axis.

\begin{figure}[htb]
  \centering
<<made,cache=FALSE,echo=FALSE,warning=FALSE>>=
s.k <- data.frame(Gen=character(), 
                  Skewness=character(),
                  Kurtosis=character(),
                  stringsAsFactors=FALSE)

for ( i in c(64,128,256)) {
    for ( j in unique(subset(made.data,Gen==i)$ID) ) {
        this.data <- subset(made.data,Gen==i & ID==j)$Fitness
        s.k <- rbind( s.k
                     , data.frame(Gen=paste("Gen",i)
                                  ,Skewness=skewness(this.data)
                                  ,Kurtosis=kurtosis(this.data)))
    }
}

ggplot(s.k,aes(x=Skewness,y=Kurtosis,color=Gen))+geom_point()+scale_x_continuous(limits=c(-1,2.5))+scale_y_continuous(limits=c(-1,8))

@ 
\caption{Skewness and kurtosis for fitness in several generations of
  the MADE problem. Different colors represent diffent generations.}
\label{fig:made}
\end{figure}

Figure \ref{fig:made} represents them for the MADE problem for which
we took measures for a variable amount of individuals every
generation, from 100 in generation 64 to around 50 in the latest
generation. A curious convergence, but without reaching, the normal
distribution is observed as generations proceed; in the first
generations, values of skewness and kurtosis are quite high and
correspond to arbitrary distribution (Beta or uniform), however, as
the simulation proceeds, values approach zero. However, they do not
converge exactly to 0, meaning that, even if uncertainty can be
approached by a normal distribution, that approximation would only be
correct for the latest simulation generations. In general, individual
fitness will follow an arbitrary distribution with a general shape and
asymmetry. 

\begin{figure}[htb]
  \centering
<<planetwars,cache=FALSE,echo=FALSE,warning=FALSE>>=
pw.s.k <- data.frame(Gen=character(), 
                  Skewness=character(),
                  Kurtosis=character(),
                  stringsAsFactors=FALSE)

for ( i in c(1,50)) {
    for ( j in unique(subset(planetwars.data,Gen==i)$ID) ) {
        this.data <- subset(planetwars.data,Gen==i & ID==j)$Fitness
        pw.s.k <- rbind( pw.s.k
                     , data.frame(Gen=paste("Gen",i)
                                  , Skewness=skewness(this.data)
                                  , Kurtosis=kurtosis(this.data)))
    }
}

ggplot(pw.s.k,aes(x=Skewness,y=Kurtosis,color=Gen))+geom_point()+scale_x_continuous(limits=c(-1,2.5))+scale_y_continuous(limits=c(-1,8))
@ 
\caption{Skewness and kurtosis for fitness in several generations of
  the planet wars problem. Different colors represent diffent generations.}
\label{fig:pw}
\end{figure}

\begin{figure}[htb]
  \centering
<<pacman,cache=FALSE,echo=FALSE,warning=FALSE>>=
pm.s.k <- data.frame(Gen=character(), 
                  Skewness=character(),
                  Kurtosis=character(),
                  stringsAsFactors=FALSE)

for ( i in c(1,25,50)) {
    for ( j in unique(subset(pacman.data,Gen==i)$ID) ) {
        this.data <- subset(pacman.data,Gen==i & ID==j)$Fitness
        pm.s.k <- rbind( pm.s.k
                        , data.frame(Gen=paste("Gen",i)
                                     , Skewness=skewness(this.data)
                                     , Kurtosis=kurtosis(this.data)))
    }
}

ggplot(pm.s.k,aes(x=Skewness,y=Kurtosis,color=Gen))+geom_point()+scale_x_continuous(limits=c(-2,10))+scale_y_continuous(limits=c(-5,100))
@ 
\caption{Skewness and kurtosis for fitness in several generations of
  the Ms. Pac-Man problem. Different colors represent diffent generations.}
\label{fig:pm}
\end{figure}

% ******************************************************************************

\section{\uppercase{Conclusions}}
\label{sec:conclusion}

\noindent TODO

\section*{\uppercase{Acknowledgements}}

\noindent This work has been supported in part by SIPESCA (Programa Operativo
FEDER de Andalucía 2007-2013), TIN2014-56494-C4-3-P (Spanish
Ministry of Economy and Competitivity), SPIP2014-01437 (Direcci{\'o}n
General de Tr{\'a}fico), PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza
Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de
Investigaci{\'o}n), and project V17-2015 of the Microprojects program 2015 from CEI BioTIC Granada.

\vfill

\bibliographystyle{apalike}
\bibliography{geneura,GA-general,noisy}

\end{document}

